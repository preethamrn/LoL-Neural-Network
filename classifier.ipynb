{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "gameData = list()\n",
    "champions = list()\n",
    "\n",
    "with open('gameData.pickle', 'rb') as f:\n",
    "    gameData = pickle.load(f)\n",
    "with open('champions.pickle', 'rb') as f:\n",
    "    champions = pickle.load(f)\n",
    "\n",
    "# convert the games into data that can be fed into Tensorflow\n",
    "games = list()\n",
    "winners = list()\n",
    "winners_categorical = list()\n",
    "roles = ['Top', 'Jungle', 'Middle', 'Support', 'ADC']\n",
    "\n",
    "for game in gameData:\n",
    "    g = np.zeros(64)\n",
    "    w = np.zeros(2)\n",
    "    stat_index = 0\n",
    "    for k, v in game.items():\n",
    "        if k.startswith('t0p'):\n",
    "            i = ['t0p_top_lane_champID', 't0p_jungle_champID', 't0p_mid_lane_champID', 't0p_support_champID', 't0p_ad_carry_champID'].index(k)\n",
    "            winrate = champions[v]['WR_'+roles[i]]\n",
    "            g[stat_index] = winrate\n",
    "        elif k.startswith('t0'):\n",
    "            g[stat_index] = v\n",
    "        elif k.startswith('t1p'):\n",
    "            i = ['t1p_top_lane_champID', 't1p_jungle_champID', 't1p_mid_lane_champID', 't1p_support_champID', 't1p_ad_carry_champID'].index(k)\n",
    "            winrate = champions[v]['WR_'+roles[i]]\n",
    "            g[stat_index] = winrate\n",
    "        elif k.startswith('t1'):\n",
    "            g[stat_index] = v\n",
    "        stat_index += 1\n",
    "    w[game['winner']] = 1\n",
    "    games.append(g)\n",
    "    winners.append(w)\n",
    "    winners_categorical.append(game['winner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.501552795031\n",
      "0.0183596872777 0.485814942171\n",
      "0.0185332673676 0.63874929291\n",
      "----FEATURE IMPORTANCES----\n",
      "1. t0_mpperlevel (0.038975)\n",
      "2. t0_mp (0.037845)\n",
      "3. t1_hp (0.037160)\n",
      "4. t1_mp (0.036926)\n",
      "5. t0_hp (0.035174)\n",
      "6. t0_attackdamage (0.033620)\n",
      "7. t1_hpperlevel (0.033266)\n",
      "8. t0_hpperlevel (0.032903)\n",
      "9. t1_mpperlevel (0.031221)\n",
      "10. t1_attackdamage (0.031047)\n",
      "11. t1_armor (0.029509)\n",
      "12. t0_armor (0.029151)\n",
      "13. t0_attackrange (0.028547)\n",
      "14. t1_attackrange (0.027568)\n",
      "15. t0p_ad_carry_champID (0.026576)\n",
      "16. t1_mpregen (0.026180)\n",
      "17. t0_mpregen (0.026032)\n",
      "18. t0p_mid_lane_champID (0.025472)\n",
      "19. t1p_mid_lane_champID (0.025154)\n",
      "20. t1p_ad_carry_champID (0.024738)\n",
      "21. t1_movespeed (0.024235)\n",
      "22. t1p_jungle_champID (0.024078)\n",
      "23. t1p_support_champID (0.023814)\n",
      "24. t1_hpregen (0.023700)\n",
      "25. t0p_jungle_champID (0.023301)\n",
      "26. t0_movespeed (0.022831)\n",
      "27. t0p_top_lane_champID (0.022698)\n",
      "28. t1p_top_lane_champID (0.022503)\n",
      "29. t0p_support_champID (0.021173)\n",
      "30. t0_attackspeedperlevel (0.020962)\n",
      "31. t1_attackspeedperlevel (0.019330)\n",
      "32. t1_armorperlevel (0.018608)\n",
      "33. t0_hpregen (0.017070)\n",
      "34. winner (0.015994)\n",
      "35. t0_armorperlevel (0.014314)\n",
      "36. t1_attackdamageperlevel (0.010031)\n",
      "37. t0_attackdamageperlevel (0.009880)\n",
      "38. t0_spellblockperlevel (0.008764)\n",
      "39. t0_spellblock (0.008073)\n",
      "40. t1_spellblock (0.007196)\n",
      "41. t1_spellblockperlevel (0.006949)\n",
      "42. t1_mpregenperlevel (0.005135)\n",
      "43. t0_hpregenperlevel (0.004996)\n",
      "44. t1_hpregenperlevel (0.003802)\n",
      "45. t0_mpregenperlevel (0.003495)\n",
      "46. t0_crit (0.000000)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-4cf2e5fb6511>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%d. %s (%f)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimportances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# using RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "\n",
    "# create test train split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(games, winners_categorical, test_size=0.30, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, oob_score=True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predicted_results = clf.predict(X_test)\n",
    "r2 = clf.score(X_test, y_test)\n",
    "\n",
    "t, t_p = kendalltau(predicted_results, y_test)\n",
    "r, r_p = spearmanr(predicted_results, y_test)\n",
    "\n",
    "print(r2)\n",
    "print(t, t_p)\n",
    "print(r, r_p)\n",
    "\n",
    "\n",
    "print('----FEATURE IMPORTANCES----')\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "features = list()\n",
    "\n",
    "for feature in gameData[0]:\n",
    "    features.append(feature)\n",
    "    \n",
    "for i, f in enumerate(features):\n",
    "    print(\"%d. %s (%f)\" % (i+1, features[indices[i]], importances[indices[i]]))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.520186\n",
      "step 100, training accuracy 0.524845\n",
      "step 200, training accuracy 0.524845\n",
      "step 300, training accuracy 0.504658\n",
      "step 400, training accuracy 0.51087\n",
      "step 500, training accuracy 0.476708\n",
      "step 600, training accuracy 0.498447\n",
      "step 700, training accuracy 0.501553\n",
      "step 800, training accuracy 0.473602\n",
      "step 900, training accuracy 0.498447\n",
      "step 1000, training accuracy 0.473602\n",
      "step 1100, training accuracy 0.493789\n",
      "step 1200, training accuracy 0.526398\n",
      "step 1300, training accuracy 0.503106\n",
      "step 1400, training accuracy 0.515528\n",
      "step 1500, training accuracy 0.468944\n",
      "step 1600, training accuracy 0.465839\n",
      "step 1700, training accuracy 0.523292\n",
      "step 1800, training accuracy 0.478261\n",
      "step 1900, training accuracy 0.517081\n",
      "test accuracy 0.531056\n"
     ]
    }
   ],
   "source": [
    "# using tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# create test train split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(games, winners, test_size=0.30, random_state=42)\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "inputs = 64\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, inputs])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "\n",
    "# first Convolutional Layer\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "x_image = tf.reshape(x, [-1,8,8,1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# second Convolutional Layer\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# densely connected Layer\n",
    "W_fc1 = weight_variable([2*2*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 2*2*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# Dropout: reduce overfitting\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# readout Layer\n",
    "W_fc2 = weight_variable([1024, 2])\n",
    "b_fc2 = bias_variable([2])\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(2000):\n",
    "    idx = np.random.choice(np.arange(len(X_train)), 100, replace=False) # get batch size of 500 w/ matching indices\n",
    "    X_train_batch = [X_train[i] for i in idx]\n",
    "    y_train_batch = [y_train[i] for i in idx]\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:X_test, y_: y_test, keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: X_train_batch, y_: y_train_batch, keep_prob: 0.5})\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={x: X_test, y_: y_test, keep_prob: 1.0}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
