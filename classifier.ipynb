{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "gameData = list()\n",
    "champions = list()\n",
    "\n",
    "with open('gameData.pickle', 'rb') as f:\n",
    "    gameData = pickle.load(f)\n",
    "with open('champions.pickle', 'rb') as f:\n",
    "    champions = pickle.load(f)\n",
    "\n",
    "# convert the games into data that can be fed into Tensorflow\n",
    "games = list()\n",
    "winners = list()\n",
    "winners_categorical = list()\n",
    "roles = ['Top', 'Jungle', 'Middle', 'Support', 'ADC']\n",
    "\n",
    "for game in gameData:\n",
    "    g = np.zeros(42)\n",
    "    w = np.zeros(2)\n",
    "    stat_index = 0\n",
    "    for k, v in game.items():\n",
    "        if k.startswith('t0p') or k.startswith('t1p'):\n",
    "            continue\n",
    "        elif k.startswith('t0'):\n",
    "            g[stat_index] = v\n",
    "            stat_index += 1\n",
    "        elif k.startswith('t1'):\n",
    "            g[stat_index] = v\n",
    "            stat_index += 1\n",
    "#    for k, v in game.items():\n",
    "#        if k.startswith('t0p'):\n",
    "#            i = ['t0p_top_lane_champID', 't0p_jungle_champID', 't0p_mid_lane_champID', 't0p_support_champID', 't0p_ad_carry_champID'].index(k)\n",
    "#            g[stat_index + len(champions)*i + champions[v]['smallID']] = 1\n",
    "#        elif k.startswith('t1p'):\n",
    "#            i = ['t1p_top_lane_champID', 't1p_jungle_champID', 't1p_mid_lane_champID', 't1p_support_champID', 't1p_ad_carry_champID'].index(k)\n",
    "#            g[stat_index + len(champions)*(5+i) + champions[v]['smallID']] = 1\n",
    "    w[game['winner']] = 1\n",
    "    games.append(g)\n",
    "    winners.append(w)\n",
    "    winners_categorical.append(game['winner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.555031446541\n",
      "0.121265291524 4.76900131289e-06\n",
      "0.122171338371 0.00202492297527\n",
      "----FEATURE IMPORTANCES----\n",
      "1. t0p_jungle_champID (0.045965)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-952f562ac26d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%d. %s (%f)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimportances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# using RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "\n",
    "# create test train split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(games, winners_categorical, test_size=0.30, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, oob_score=True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predicted_results = clf.predict(X_test)\n",
    "r2 = clf.score(X_test, y_test)\n",
    "\n",
    "t, t_p = kendalltau(predicted_results, y_test)\n",
    "r, r_p = spearmanr(predicted_results, y_test)\n",
    "\n",
    "print(r2)\n",
    "print(t, t_p)\n",
    "print(r, r_p)\n",
    "\n",
    "\n",
    "print('----FEATURE IMPORTANCES----')\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "features = list()\n",
    "\n",
    "#for i in range(1340):\n",
    "#    features.append(['t0p_top_lane_champID', 't0p_jungle_champID', 't0p_mid_lane_champID', \n",
    "#                     't0p_support_champID', 't0p_ad_carry_champID','t1p_top_lane_champID', \n",
    "#                     't1p_jungle_champID', 't1p_mid_lane_champID', 't1p_support_champID', 't1p_ad_carry_champID'][int(i/134)] + str(i%134))\n",
    "for feature in gameData[0]:\n",
    "    if feature.startswith('t0p') or feature.startswith('t1p'):\n",
    "        features.append(feature)\n",
    "    \n",
    "for i, f in enumerate(features):\n",
    "    print(\"%d. %s (%f)\" % (i+1, features[indices[i]], importances[indices[i]]))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.512579\n",
      "step 100, training accuracy 0.487421\n",
      "step 200, training accuracy 0.514151\n",
      "step 300, training accuracy 0.490566\n",
      "step 400, training accuracy 0.493711\n",
      "step 500, training accuracy 0.487421\n",
      "step 600, training accuracy 0.512579\n",
      "step 700, training accuracy 0.515723\n",
      "step 800, training accuracy 0.5\n",
      "step 900, training accuracy 0.509434\n",
      "step 1000, training accuracy 0.498428\n",
      "step 1100, training accuracy 0.498428\n",
      "step 1200, training accuracy 0.477987\n",
      "step 1300, training accuracy 0.5\n",
      "step 1400, training accuracy 0.492138\n",
      "step 1500, training accuracy 0.514151\n",
      "step 1600, training accuracy 0.506289\n",
      "step 1700, training accuracy 0.496855\n",
      "step 1800, training accuracy 0.466981\n",
      "step 1900, training accuracy 0.47956\n",
      "test accuracy 0.493711\n"
     ]
    }
   ],
   "source": [
    "# using tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# create test train split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(games, winners, test_size=0.30, random_state=42)\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 64]) # the 64 depends on the number of inputs (ie. size of game)\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "\n",
    "# first Convolutional Layer\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "x_image = tf.reshape(x, [-1,8,8,1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# second Convolutional Layer\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# densely connected Layer\n",
    "W_fc1 = weight_variable([2*2*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 2*2*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# Dropout: reduce overfitting\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# readout Layer\n",
    "W_fc2 = weight_variable([1024, 2])\n",
    "b_fc2 = bias_variable([2])\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(2000):\n",
    "    idx = np.random.choice(np.arange(len(X_train)), 100, replace=False) # get batch size of 100 w/ matching indices\n",
    "    X_train_batch = [X_train[i] for i in idx]\n",
    "    y_train_batch = [y_train[i] for i in idx]\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:X_test, y_: y_test, keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: X_train_batch, y_: y_train_batch, keep_prob: 0.5})\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={x: X_test, y_: y_test, keep_prob: 1.0}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.487421\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# create test train split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(games, winners, test_size=0.30, random_state=42)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 42])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "\n",
    "W = tf.Variable(tf.zeros([42,2]))\n",
    "b = tf.Variable(tf.zeros([2]))\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "y = tf.matmul(x,W) + b\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "for _ in range(1000):\n",
    "    idx = np.random.choice(np.arange(len(X_train)), 100, replace=False) # get batch size of 100 w/ matching indices\n",
    "    X_train_batch = [X_train[i] for i in idx]\n",
    "    y_train_batch = [y_train[i] for i in idx]\n",
    "    train_step.run(feed_dict={x: X_train_batch, y_: y_train_batch, keep_prob: 0.5})\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(accuracy.eval(feed_dict={x: X_test, y_: y_test, keep_prob: 1.0}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.67500000e+03,   1.65354331e-01,   0.00000000e+00,\n",
       "         1.57500000e+03,   2.90000000e+01,   1.50000000e+01,\n",
       "         0.00000000e+00,   1.70000000e+01,   1.00000000e+00,\n",
       "         4.28000000e+02,   1.14900000e+03,   0.00000000e+00,\n",
       "         1.96000000e+02,   1.52000000e+02,   1.00000000e+00,\n",
       "         1.49700000e+03,   1.13000000e+02,   1.60000000e+01,\n",
       "         0.00000000e+00,   1.60000000e+01,   0.00000000e+00,\n",
       "         3.30000000e+01,   3.00000000e+00,   1.69000000e+03,\n",
       "         2.82000000e+03,   1.20000000e+01,   2.77000000e+02,\n",
       "         7.70000000e+01,   1.22000000e+02,   0.00000000e+00,\n",
       "         1.56000000e+02,   2.65000000e+02,   1.40000000e+02,\n",
       "         2.70700000e+03,   0.00000000e+00,   7.00000000e+00,\n",
       "         0.00000000e+00,   4.02777778e-01,   0.00000000e+00,\n",
       "         1.50000000e+01,   2.25000000e+03,   4.12000000e+02])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "games[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
